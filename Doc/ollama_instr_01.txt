В терминале или подключаемся к серверу через PuTTY 

Шаги по инструкции: https://github.com/jmorganca/ollama/blob/main/docs/linux.md
Download the ollama binary
sudo curl -L https://ollama.ai/download/ollama-linux-amd64 -o /usr/bin/ollama
sudo chmod +x /usr/bin/ollama

Create a user for Ollama:
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama

Create a service file:
используем редактор nano
sudo nano /etc/systemd/system/ollama.service

Содержимое файла:
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3

[Install]
WantedBy=default.target

сохранить файл: Ctrl+O
выйти из редактора: Ctrl+X

Then start the service:
sudo systemctl daemon-reload
sudo systemctl enable ollama

Установить CUDA, cuDNN для вашей видео карты.

Создать папки (команда mkdir), например такие:
ollama_test
           |
		   models
		   |
		   IlyaGusev_saiga_mistral_7b_gguf

Скачать файл модели model-q8_0.gguf
 https://huggingface.co/IlyaGusev/saiga_mistral_7b_gguf/tree/main
и поместить его в папку ollama_test/models/IlyaGusev_saiga_mistral_7b_gguf

Перейти в паку
cd ollama_test/models/IlyaGusev_saiga_mistral_7b_gguf

Создать файл Modelfile:
используем редактор nano
sudo nano Modelfile

содержимое файла:

FROM ./model-q8_0.gguf

сохранить файл: Ctrl+O
выйти из редактора: Ctrl+X

Create the model in Ollama:
ollama create saiga_mistral_7b -f Modelfile

Проверяем список моделей:
ollama list

стартуем ollama:
sudo systemctl start ollama

Запускаем модель в коммандной строке: 
ollama run saiga_mistral_7b

выходим из режима коммандной сроки:
/bye

How can I expose Ollama on my network?
Шаги по иструкции https://github.com/jmorganca/ollama/blob/main/docs/faq.md#how-can-i-expose-ollama-on-my-network

sudo mkdir -p /etc/systemd/system/ollama.service.d

sudo nano /etc/systemd/system/ollama.service.d/environment.conf

содержимое файла:

[Service]
Environment="OLLAMA_HOST=0.0.0.0:11434"

сохранить файл: Ctrl+O
выйти из редактора: Ctrl+X

Reload systemd and restart Ollama:

sudo systemctl daemon-reload
sudo systemctl restart ollama

Проверить:
sudo ufw status

sudo ufw allow ollama
sudo ufw allow 11434/tcp

Проверка:
В файле https://github.com/kvoloshenko/LMRAG_01/blob/main/ollama_test_01.py
исправить localhost на ip адрес вашего сервера в строке
url = "http://localhost:11434/api/generate"
и выполнить этот файл


